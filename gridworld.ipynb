{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row Gridword Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldRow():\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.nS = size # state number\n",
    "        self.nA = 2 # action number (right, left)\n",
    "        self.MAX_X = size-1 # max state, we start to index from 0\n",
    "        P = {} # set up the environment {p:{a:[(p(s'|s,a), s', r(s,a,s'), done)]}\n",
    "\n",
    "        for s in range(self.nS):\n",
    "\n",
    "            dynamic_s = {}\n",
    "\n",
    "            for a in range(self.nA):\n",
    "\n",
    "                s_prime_list = []\n",
    "                p = 1 if s != 0 and s != self.nS-1 else 0\n",
    "\n",
    "                if a == 0:\n",
    "                    s_prime = max(0, s-1)\n",
    "                else:\n",
    "                    s_prime = min(self.MAX_X, s+1)\n",
    "                \n",
    "                if s_prime == 0:\n",
    "                    reward = -100\n",
    "                    done = True\n",
    "                elif s_prime == self.MAX_X:\n",
    "                    reward = 10\n",
    "                    done = True\n",
    "                else:\n",
    "                    reward = 0\n",
    "                    done = False\n",
    "                \n",
    "                s_prime_list.append((p, s_prime, reward, done))\n",
    "                dynamic_s.update({a:s_prime_list})\n",
    "\n",
    "            P.update({s: dynamic_s})\n",
    "\n",
    "        self.P = P # assign the environment\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.P}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: [(0, 0, -100, True)], 1: [(0, 1, 0, False)]}, 1: {0: [(1, 0, -100, True)], 1: [(1, 2, 0, False)]}, 2: {0: [(1, 1, 0, False)], 1: [(1, 3, 0, False)]}, 3: {0: [(1, 2, 0, False)], 1: [(1, 4, 10, True)]}, 4: {0: [(0, 3, 0, False)], 1: [(0, 4, 10, True)]}}\n"
     ]
    }
   ],
   "source": [
    "env = GridWorldRow()\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.ones([env.nS, env.nA]) * 0.5\n",
    "V = np.zeros([env.nS, 1])\n",
    "\n",
    "gamma = 0.99 #replacement factor the return\n",
    "theta = 1e-5 #similarity threshold to stop update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_value_for_s_a(env, V, s, a, gamma):\n",
    "    q = 0\n",
    "    for (p_sPrime, sPrime, r_ss_a, done) in env.P[s][a]:\n",
    "        q += p_sPrime * (r_ss_a + gamma * V[sPrime])\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done after 22 iterations\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    delta = 0\n",
    "\n",
    "    for s in range(env.nS):\n",
    "        V_new = 0\n",
    "\n",
    "        for a in range(env.nA):\n",
    "            prob_a = pi[s][a]\n",
    "            q_s_a = compute_q_value_for_s_a(env, V, s, a, gamma)\n",
    "            V_new += prob_a * q_s_a\n",
    "        \n",
    "        delta = max(delta, np.abs(V_new - V[s]))\n",
    "        V[s] = V_new\n",
    "\n",
    "    if delta < theta:\n",
    "        print(f\"Done after {i} iterations\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ],\n",
       "       [-71.62196676],\n",
       "       [-43.6807471 ],\n",
       "       [-16.62196981],\n",
       "       [  0.        ]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(env.nS):\n",
    "    q_s = np.zeros([env.nA, 1])\n",
    "\n",
    "    for a in range(env.nA):\n",
    "        q_s[a] = compute_q_value_for_s_a(env, V, s, a, gamma)\n",
    "\n",
    "    best_a = np.argmax(q_s)\n",
    "    pi[s] = np.eye(env.nA)[best_a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, pi, V, gamma, theta):\n",
    "    V_updated = np.copy(V)\n",
    "    improved = True\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        for s in range(env.nS):\n",
    "            V_new = 0\n",
    "\n",
    "            for a in range(env.nA):\n",
    "                prob_a = pi[s][a]\n",
    "                q_s_a = compute_q_value_for_s_a(env, V, s, a, gamma)\n",
    "                V_new += prob_a * q_s_a\n",
    "            \n",
    "            delta = max(delta, np.abs(V_new - V_updated[s]))\n",
    "            V_updated[s] = V_new\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    if (np.array_equal(V, V_updated)):\n",
    "        improved = False\n",
    "\n",
    "    return V_updated, improved\n",
    "\n",
    "def improve_policy(env, pi, V, gamma):\n",
    "    for s in range(env.nS):\n",
    "        q_s = np.zeros([env.nA, 1])\n",
    "\n",
    "        for a in range(env.nA):\n",
    "             q_s[a] = compute_q_value_for_s_a(env, V, s, a, gamma)\n",
    "\n",
    "        best_a = np.argmax(q_s)\n",
    "        pi[s] = np.eye(env.nA)[best_a]\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, pi, V, gamma, theta):\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        V, improved = evaluate_policy(env, pi, V, gamma, theta)\n",
    "        pi = improve_policy(env, pi, V, gamma)\n",
    "\n",
    "        if improved == False:\n",
    "            print(f\"Done after {i} step\")\n",
    "            break\n",
    "    return pi, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.ones([env.nS, env.nA]) * 0.5\n",
    "V = np.zeros([env.nS, 1])\n",
    "\n",
    "gamma = 0.99 #replacement factor the return\n",
    "theta = 1e-5 #similarity threshold to stop update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done after 5 step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi, V = policy_iteration(env, pi, V, gamma, theta)\n",
    "pi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
